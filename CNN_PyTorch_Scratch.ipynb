{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smit-1z/DataMiningTermProject/blob/main/CNN_PyTorch_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/gaurav67890/Pytorch_Tutorials/blob/master/cnn-scratch-training.ipynb\n",
        "\n",
        "https://github.com/gaurav67890/Pytorch_Tutorials/blob/master/cnn-scratch-inference.ipynb"
      ],
      "metadata": {
        "id": "y7TFbaOg8IU1"
      },
      "id": "y7TFbaOg8IU1"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "79d38a59",
      "metadata": {
        "id": "79d38a59"
      },
      "outputs": [],
      "source": [
        "#Load libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "from torchvision.models import squeezenet1_1\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import glob\n",
        "import pathlib\n",
        "from io import open\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Ignore the warning\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8339bddf",
      "metadata": {
        "id": "8339bddf"
      },
      "outputs": [],
      "source": [
        "#checking for device\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dea0b70d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dea0b70d",
        "outputId": "40a20ce7-5ffb-46fd-c8cd-f65b17cc1542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pszhTB-HcGwe",
        "outputId": "92e5d895-cc64-4192-f635-1d7490a2a68f"
      },
      "id": "pszhTB-HcGwe",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Path for training and testing directory\n",
        "\n",
        "train_path='/content/drive/MyDrive/ImageDataSet/Train'\n",
        "test_path='/content/drive/MyDrive/ImageDataSet/Test'\n",
        "pred_path='/content/drive/MyDrive/ImageDataSet/Dev'\n"
      ],
      "metadata": {
        "id": "V6uMP2sqDxM5"
      },
      "id": "V6uMP2sqDxM5",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0efb1cc6",
      "metadata": {
        "id": "0efb1cc6"
      },
      "outputs": [],
      "source": [
        "#Transforms\n",
        "transformer=transforms.Compose([\n",
        "    transforms.Resize((150,150)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
        "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
        "                        [0.5,0.5,0.5])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ddff9516",
      "metadata": {
        "id": "ddff9516"
      },
      "outputs": [],
      "source": [
        "#Dataloader\n",
        "train_loader=DataLoader(\n",
        "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
        "    batch_size=128, shuffle=True\n",
        ")\n",
        "test_loader=DataLoader(\n",
        "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
        "    batch_size=128, shuffle=True\n",
        ")\n",
        "\n",
        "#changed batch size from 256 to 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "55f446e2",
      "metadata": {
        "id": "55f446e2"
      },
      "outputs": [],
      "source": [
        "#categories\n",
        "root=pathlib.Path(train_path)\n",
        "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "89bdd035",
      "metadata": {
        "id": "89bdd035",
        "outputId": "e8777180-555a-4923-c71e-9d48cee05afa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cheetah', 'fox', 'hyena', 'lion', 'tiger', 'wolf']\n"
          ]
        }
      ],
      "source": [
        "print(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "b7d3c44e",
      "metadata": {
        "id": "b7d3c44e"
      },
      "outputs": [],
      "source": [
        "#CNN Network\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self,num_classes=6):\n",
        "        super(ConvNet,self).__init__()\n",
        "        \n",
        "        #Output size after convolution filter\n",
        "        #((w-f+2P)/s) +1\n",
        "        \n",
        "        #Input shape= (256,3,150,150)\n",
        "        \n",
        "        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
        "        #Shape= (256,12,150,150)\n",
        "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
        "        #Shape= (256,12,150,150)\n",
        "        self.relu1=nn.ReLU()\n",
        "        #Shape= (256,12,150,150)\n",
        "        \n",
        "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
        "        #Reduce the image size be factor 2\n",
        "        #Shape= (256,12,75,75)\n",
        "        \n",
        "        \n",
        "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
        "        #Shape= (256,20,75,75)\n",
        "        self.relu2=nn.ReLU()\n",
        "        #Shape= (256,20,75,75)\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
        "        #Shape= (256,32,75,75)\n",
        "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
        "        #Shape= (256,32,75,75)\n",
        "        self.relu3=nn.ReLU()\n",
        "        #Shape= (256,32,75,75)\n",
        "        \n",
        "        \n",
        "        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\n",
        "        \n",
        "        \n",
        "        \n",
        "        #Feed forwad function\n",
        "        \n",
        "    def forward(self,input):\n",
        "        output=self.conv1(input)\n",
        "        output=self.bn1(output)\n",
        "        output=self.relu1(output)\n",
        "            \n",
        "        output=self.pool(output)\n",
        "            \n",
        "        output=self.conv2(output)\n",
        "        output=self.relu2(output)\n",
        "            \n",
        "        output=self.conv3(output)\n",
        "        output=self.bn3(output)\n",
        "        output=self.relu3(output)\n",
        "            \n",
        "            \n",
        "            #Above output will be in matrix form, with shape (256,32,75,75)\n",
        "            \n",
        "        output=output.view(-1,32*75*75)\n",
        "            \n",
        "            \n",
        "        output=self.fc(output)\n",
        "            \n",
        "        return output\n",
        "            \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "978d3107",
      "metadata": {
        "id": "978d3107"
      },
      "outputs": [],
      "source": [
        "model=ConvNet(num_classes=6).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "e8d05106",
      "metadata": {
        "id": "e8d05106"
      },
      "outputs": [],
      "source": [
        "#Optimizer and loss function\n",
        "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
        "loss_function=nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "5f5f66fb",
      "metadata": {
        "id": "5f5f66fb"
      },
      "outputs": [],
      "source": [
        "num_epochs=15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "355df580",
      "metadata": {
        "id": "355df580"
      },
      "outputs": [],
      "source": [
        "#calculating the size of training and testing images\n",
        "train_count=len(glob.glob(train_path+'/**/*.png'))\n",
        "test_count=len(glob.glob(test_path+'/**/*.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "c420e6b9",
      "metadata": {
        "id": "c420e6b9",
        "outputId": "72f4c881-9bce-44d4-c7f9-0b2d87644dd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1457 176\n"
          ]
        }
      ],
      "source": [
        "print(train_count,test_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "2d78d5b3",
      "metadata": {
        "id": "2d78d5b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81848f8e-530c-4575-b788-36c33e7e7889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Train Loss: tensor(27.0329) Train Accuracy: 0.23747426218256693 Test Accuracy: 0.13636363636363635\n",
            "Epoch: 1 Train Loss: tensor(11.5391) Train Accuracy: 0.4344543582704187 Test Accuracy: 0.21022727272727273\n",
            "Epoch: 2 Train Loss: tensor(6.0168) Train Accuracy: 0.49759780370624573 Test Accuracy: 0.30113636363636365\n",
            "Epoch: 3 Train Loss: tensor(2.9282) Train Accuracy: 0.6245710363761153 Test Accuracy: 0.25\n",
            "Epoch: 4 Train Loss: tensor(1.5536) Train Accuracy: 0.7590940288263556 Test Accuracy: 0.4715909090909091\n",
            "Epoch: 5 Train Loss: tensor(0.7447) Train Accuracy: 0.8490048043925875 Test Accuracy: 0.5\n",
            "Epoch: 6 Train Loss: tensor(0.4346) Train Accuracy: 0.8963623884694578 Test Accuracy: 0.45454545454545453\n",
            "Epoch: 7 Train Loss: tensor(0.5220) Train Accuracy: 0.8846945778997941 Test Accuracy: 0.4943181818181818\n",
            "Epoch: 8 Train Loss: tensor(0.1800) Train Accuracy: 0.9389155799588195 Test Accuracy: 0.5056818181818182\n",
            "Epoch: 9 Train Loss: tensor(0.1222) Train Accuracy: 0.9629375428963624 Test Accuracy: 0.4659090909090909\n",
            "Epoch: 10 Train Loss: tensor(0.1527) Train Accuracy: 0.9615648592999314 Test Accuracy: 0.5056818181818182\n",
            "Epoch: 11 Train Loss: tensor(0.1036) Train Accuracy: 0.9684282772820865 Test Accuracy: 0.5397727272727273\n",
            "Epoch: 12 Train Loss: tensor(0.0700) Train Accuracy: 0.9759780370624571 Test Accuracy: 0.5284090909090909\n",
            "Epoch: 13 Train Loss: tensor(0.0587) Train Accuracy: 0.9821551132463967 Test Accuracy: 0.5\n",
            "Epoch: 14 Train Loss: tensor(0.0324) Train Accuracy: 0.9931365820178449 Test Accuracy: 0.5511363636363636\n"
          ]
        }
      ],
      "source": [
        "#Model training and saving best model\n",
        "\n",
        "best_accuracy=0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    #Evaluation and training on training dataset\n",
        "    model.train()\n",
        "    train_accuracy=0.0\n",
        "    train_loss=0.0\n",
        "    \n",
        "    for i, (images,labels) in enumerate(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            images=Variable(images.cuda())\n",
        "            labels=Variable(labels.cuda())\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs=model(images)\n",
        "        loss=loss_function(outputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        train_loss+= loss.cpu().data*images.size(0)\n",
        "        _,prediction=torch.max(outputs.data,1)\n",
        "        \n",
        "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
        "        \n",
        "    train_accuracy=train_accuracy/train_count\n",
        "    train_loss=train_loss/train_count\n",
        "    \n",
        "    \n",
        "    # Evaluation on testing dataset\n",
        "    model.eval()\n",
        "    \n",
        "    test_accuracy=0.0\n",
        "    for i, (images,labels) in enumerate(test_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            images=Variable(images.cuda())\n",
        "            labels=Variable(labels.cuda())\n",
        "            \n",
        "        outputs=model(images)\n",
        "        _,prediction=torch.max(outputs.data,1)\n",
        "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
        "    \n",
        "    test_accuracy=test_accuracy/test_count\n",
        "    \n",
        "    \n",
        "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
        "    \n",
        "    #Save the best model\n",
        "    if test_accuracy>best_accuracy:\n",
        "        torch.save(model.state_dict(),'best_checkpoint.model')\n",
        "        best_accuracy=test_accuracy\n",
        "    \n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('best_checkpoint.model')\n",
        "model=ConvNet(num_classes=6)\n",
        "model.load_state_dict(checkpoint)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "MKQUaOOBh8Gw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ba88bc-503a-450f-e82f-cc7b11ba2e13"
      },
      "id": "MKQUaOOBh8Gw",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvNet(\n",
              "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu1): ReLU()\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu2): ReLU()\n",
              "  (conv3): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu3): ReLU()\n",
              "  (fc): Linear(in_features=180000, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transforms #2 \n",
        "transformer=transforms.Compose([\n",
        "    transforms.Resize((150,150)),\n",
        "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
        "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
        "                        [0.5,0.5,0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "_zKPHonYjHIs"
      },
      "id": "_zKPHonYjHIs",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(img_path,transformer):\n",
        "  image=Image.open(img_path).convert('RGB')\n",
        "  image_tensor = transformer(image).float()\n",
        "  image_tensor=image_tensor.unsqueeze_(0)\n",
        "  if torch.cuda.is_available():\n",
        "      image_tensor.cuda()\n",
        "        \n",
        "  input=Variable(image_tensor)\n",
        "  output=model(input)\n",
        "  index=output.data.numpy().argmax()\n",
        "  pred=classes[index]\n",
        "    \n",
        "  return pred"
      ],
      "metadata": {
        "id": "geVfhZ5mjKST"
      },
      "id": "geVfhZ5mjKST",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_path=glob.glob(pred_path+'/*.png')\n",
        "\n",
        "print(images_path[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgovmwjAj4CI",
        "outputId": "39573535-3a04-432e-f7bd-ad09c82ab72b"
      },
      "id": "xgovmwjAj4CI",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ImageDataSet/Dev/00000571_224resized.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_dict={}\n",
        "\n",
        "for i in images_path:\n",
        "  pred_dict[i[i.rfind('/')+1:]] = prediction(i,transformer)"
      ],
      "metadata": {
        "id": "0bboNVDmj_Ap"
      },
      "id": "0bboNVDmj_Ap",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onu2qFr7l8W9",
        "outputId": "9b134315-60d7-430b-dfeb-4ca0c5ab448f"
      },
      "id": "onu2qFr7l8W9",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'00000571_224resized.png': 'cheetah',\n",
              " '00000568_224resized.png': 'tiger',\n",
              " '00000567_224resized.png': 'tiger',\n",
              " '00000564_224resized.png': 'tiger',\n",
              " '00000563_224resized.png': 'cheetah',\n",
              " '00000547_224resized.png': 'cheetah',\n",
              " '00000549_224resized.png': 'hyena',\n",
              " '00000580_224resized.png': 'tiger',\n",
              " '00000569_224resized.png': 'cheetah',\n",
              " '00000562_224resized.png': 'cheetah',\n",
              " '00000596_224resized.png': 'hyena',\n",
              " '00000593_224resized.png': 'cheetah',\n",
              " '00000587_224resized.png': 'cheetah',\n",
              " '00000595_224resized.png': 'cheetah',\n",
              " '00000419_224resized.png': 'fox',\n",
              " '00000440_224resized.png': 'fox',\n",
              " '00000431_224resized.png': 'hyena',\n",
              " '00000426_224resized.png': 'fox',\n",
              " '00000479_224resized.png': 'fox',\n",
              " '00000475_224resized.png': 'hyena',\n",
              " '00000487_224resized.png': 'fox',\n",
              " '00000442_224resized.png': 'fox',\n",
              " '00000445_224resized.png': 'fox',\n",
              " '00000483_224resized.png': 'cheetah',\n",
              " '00000425_224resized.png': 'wolf',\n",
              " '00000465_224resized.png': 'fox',\n",
              " '00000421_224resized.png': 'cheetah',\n",
              " '00000470_224resized.png': 'cheetah',\n",
              " '00000478_224resized.png': 'fox',\n",
              " '00000580_224resized copy.png': 'lion',\n",
              " '00000548_224resized.png': 'cheetah',\n",
              " '00000583_224resized.png': 'tiger',\n",
              " '00000520_224resized.png': 'hyena',\n",
              " '00000581_224resized.png': 'fox',\n",
              " '00000587_224resized copy.png': 'lion',\n",
              " '00000511_224resized.png': 'hyena',\n",
              " '00000514_224resized.png': 'hyena',\n",
              " '00000509_224resized.png': 'fox',\n",
              " '00000574_224resized.png': 'cheetah',\n",
              " '00000571_224resized copy.png': 'hyena',\n",
              " '00000555_224resized.png': 'hyena',\n",
              " '00000531_224resized.png': 'hyena',\n",
              " '00000559_224resized.png': 'hyena',\n",
              " '00000524_224resized.png': 'wolf',\n",
              " '00000498_224resized.png': 'lion',\n",
              " '00000563_224resized copy.png': 'cheetah',\n",
              " '00000561_224resized.png': 'fox',\n",
              " '00000556_224resized.png': 'wolf',\n",
              " '00000101_224resized.png': 'lion',\n",
              " '00000560_224resized.png': 'fox',\n",
              " '00000519_224resized.png': 'lion',\n",
              " '00000518_224resized.png': 'hyena',\n",
              " '00000503_224resized.png': 'fox',\n",
              " '00000566_224resized.png': 'hyena',\n",
              " '00000584_224resized.png': 'lion',\n",
              " '00000497_224resized.png': 'fox',\n",
              " '00000562_224resized copy.png': 'lion',\n",
              " '00000523_224resized.png': 'hyena',\n",
              " '00000504_224resized.png': 'lion',\n",
              " '00000035_224resized.png': 'tiger',\n",
              " '00000042_224resized.png': 'tiger',\n",
              " '00000075_224resized.png': 'wolf',\n",
              " '00000038_224resized.png': 'tiger',\n",
              " '00000039_224resized.png': 'tiger',\n",
              " '00000041_224resized.png': 'tiger',\n",
              " '00000047_224resized.png': 'tiger',\n",
              " '00000050_224resized.png': 'tiger',\n",
              " '00000071_224resized.png': 'tiger',\n",
              " '00000037_224resized.png': 'cheetah',\n",
              " '00000032_224resized.png': 'tiger',\n",
              " '00000158_224resized.png': 'hyena',\n",
              " '00000027_224resized.png': 'cheetah',\n",
              " '00000048_224resized.png': 'tiger',\n",
              " '00000076_224resized.png': 'tiger',\n",
              " '00000029_224resized.png': 'cheetah',\n",
              " '00000540_224resized.png': 'wolf',\n",
              " '00000519_224resized copy.png': 'wolf',\n",
              " '00000474_224resized.png': 'hyena',\n",
              " '00000538_224resized.png': 'cheetah',\n",
              " '00000508_224resized.png': 'fox',\n",
              " '00000503_224resized copy.png': 'tiger',\n",
              " '00000514_224resized copy.png': 'wolf',\n",
              " '00000477_224resized.png': 'wolf',\n",
              " '00000473_224resized.png': 'hyena',\n",
              " '00000497_224resized copy.png': 'fox',\n",
              " '00000537_224resized.png': 'wolf',\n",
              " '00000512_224resized.png': 'wolf',\n",
              " '00000516_224resized.png': 'wolf',\n",
              " '00000489_224resized.png': 'wolf',\n",
              " '00000507_224resized.png': 'wolf'}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}